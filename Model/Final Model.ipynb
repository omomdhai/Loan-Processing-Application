{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modeling Dataset \n",
    "\n",
    "df = pd.read_csv('Modeling dataset.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropping id column \n",
    "\n",
    "df.drop(columns=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numeric columns in dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram plots of the numeric columns:\n",
    "\n",
    "df[numeric_cols].hist(bins=15, figsize=(15, 10))\n",
    "plt.suptitle('Histograms of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan_status plot\n",
    "\n",
    "column_name = 'loan_status' \n",
    "\n",
    "value_counts = df[column_name].value_counts()\n",
    "value_counts.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title(f'Value Counts of {column_name}')\n",
    "plt.xlabel(column_name)\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On the basis of categories provided and understanding of those terms, converting \n",
    "\n",
    "df['target'] = df['loan_status'].apply(lambda x: 1 if x in ['Current', 'Fully Paid'] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['loan_status'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'earliest_cr_line' column to datetime\n",
    "df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], errors='coerce')\n",
    "\n",
    "\n",
    "print(f\"Data type after conversion: {df['earliest_cr_line'].dtype}\")\n",
    "\n",
    "\n",
    "print(df['earliest_cr_line'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earliest_cr_line = df['earliest_cr_line']\n",
    "\n",
    "# Finding min and max dates in dataset\n",
    "min_date = earliest_cr_line.min()\n",
    "max_date = earliest_cr_line.max()\n",
    "\n",
    "print(f\"Minimum date: {min_date}\")\n",
    "print(f\"Maximum date: {max_date}\")\n",
    "\n",
    "years = earliest_cr_line.dt.year\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "years.hist(bins=30, edgecolor='black')\n",
    "plt.title('Distribution of Earliest Credit Line Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the number of years since the earliest credit line date\n",
    "from datetime import datetime\n",
    "current_date = datetime.now()\n",
    "df['credit_age_years'] = (current_date - df['earliest_cr_line']).dt.days / 365.25\n",
    "\n",
    "\n",
    "df = df.drop(columns=['earliest_cr_line'])\n",
    "\n",
    "# DataFrame with the new feature\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PURPOSE-  one hot encoding\n",
    "\n",
    "df['purpose'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_name = 'purpose'  \n",
    "\n",
    "\n",
    "if df[column_name].dtype == 'object':\n",
    "    # For categorical columns\n",
    "    value_counts = df[column_name].value_counts()\n",
    "    value_counts.plot(kind='bar', figsize=(10, 6))\n",
    "    plt.title(f'Value Counts of {column_name}')\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Counts')\n",
    "else:\n",
    "    # For numeric columns\n",
    "    df[column_name].plot(kind='bar', figsize=(10, 6))\n",
    "    plt.title(f'Bar Graph of {column_name}')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel(column_name)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping dictionary according to categories in form\n",
    "purpose_mapping = {\n",
    "    'debt_consolidation': 'debt',\n",
    "    'credit_card': 'debt',\n",
    "    'other': 'personal',\n",
    "    'home_improvement': 'home loan',\n",
    "    'small_business': 'personal',\n",
    "    'major_purchase': 'personal',\n",
    "    'car': 'personal',\n",
    "    'wedding': 'personal',\n",
    "    'medical': 'personal',\n",
    "    'house': 'home loan',\n",
    "    'moving': 'personal',\n",
    "    'vacation': 'personal',\n",
    "    'educational': 'education loan',\n",
    "    'renewable_energy': 'home loan'\n",
    "}\n",
    "\n",
    "\n",
    "df['broad_purpose'] = df['purpose'].map(purpose_mapping)\n",
    "df['broad_purpose'].value_counts()\n",
    "df.drop(columns=['purpose'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing one-hot encoding on the 'purpose' column\n",
    "\n",
    "df = pd.get_dummies(df, columns=['broad_purpose'], prefix='broad_purpose', dtype=int)\n",
    "\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DESCRIPTION - extracing some meaningful keywords ['borrow','credit','debt'] from the text and using them as new features\n",
    "\n",
    "df['desc'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing for desc\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "# Replacing NaN values in 'description' column with an empty string\n",
    "df['desc_new'] = df['desc'].fillna('')\n",
    "\n",
    "# function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Check if the text is a string\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r'<br/>', ' ', text)  # Replace HTML line breaks with spaces\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "        tokens = word_tokenize(text)  # Tokenize text\n",
    "        tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Apply preprocessing to the 'description' column\n",
    "df['cleaned_description'] = df['desc_new'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pandas.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the predefined words\n",
    "predefined_words = ['borrow', 'credit', 'debt']\n",
    "\n",
    "# Create a function to extract features for predefined words\n",
    "def extract_features(tokens, words):\n",
    "    features = {}\n",
    "    for word in words:\n",
    "        features[word] = int(word in tokens)\n",
    "    return features\n",
    "\n",
    "# Apply the feature extraction\n",
    "df_features = df['cleaned_description'].apply(lambda x: extract_features(x, predefined_words))\n",
    "\n",
    "# Convert the features to a DataFrame\n",
    "features_df = pd.DataFrame(list(df_features))\n",
    "\n",
    "# Concatenate with the original dataframe\n",
    "df_final = pd.concat([df, features_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.drop(columns=['desc'], inplace=True)\n",
    "df_final.drop(columns=['desc_new'], inplace=True)\n",
    "df_final.drop(columns=['cleaned_description'], inplace=True)\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding debt_to_income_ratio as a new feature\n",
    "\n",
    "df_final['debt_to_income_ratio'] = df_final['loan_amnt'] / df_final['annual_inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = 'target' \n",
    "\n",
    "\n",
    "value_counts = df[column_name].value_counts()\n",
    "value_counts.plot(kind='bar', figsize=(6, 4))\n",
    "plt.title(f'Value Counts of {column_name}')\n",
    "plt.xlabel(column_name)\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "target_column = 'target'\n",
    "\n",
    "# Getting the value counts of 0 and 1 in the target column\n",
    "value_counts = df_final[target_column].value_counts()\n",
    "\n",
    "# Calculating the total number of rows\n",
    "total_count = len(df)\n",
    "\n",
    "# Calculating the percentages\n",
    "percentage_0 = (value_counts[0] / total_count) * 100\n",
    "percentage_1 = (value_counts[1] / total_count) * 100\n",
    "\n",
    "#  values and percentages\n",
    "print(f\"Number of 1s: {value_counts[1]} ({percentage_1:.2f}%)\")\n",
    "print(f\"Number of 0s: {value_counts[0]} ({percentage_0:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=df_final.corr()\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the heatmap\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "sns.heatmap(data=d1,yticklabels=True,cbar=True,annot=True,cmap='viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries used\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom identity transformer for binary features\n",
    "\n",
    "def identity_transform(x):\n",
    "    return x\n",
    "    \n",
    "target_column = 'target'\n",
    "X = df_final.drop(columns=[target_column])\n",
    "y = df_final[target_column]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific features for scaling (except binary)\n",
    "\n",
    "features_for_scaling = ['loan_amnt', 'emp_length', 'annual_inc', 'delinq_2yrs', 'inq_last_6mths', 'mths_since_last_delinq', 'mths_since_last_record', 'open_acc', 'revol_bal', 'revol_util', 'total_acc', 'credit_age_years', 'debt_to_income_ratio']  # replace with your actual feature names\n",
    "binary_features = [col for col in X.columns if col not in features_for_scaling]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColumnTransformer that only scales specified features\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), features_for_scaling),\n",
    "        ('bin', FunctionTransformer(identity_transform, validate=False), binary_features )# ye line hata k bhi same aana chahiye\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline with SMOTETomek, ColumnTransformer, and LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('smotetomek', SMOTETomek(sampling_strategy='auto', random_state=42)),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('logreg', LogisticRegression(penalty='l2', solver='liblinear'))\n",
    "])\n",
    "\n",
    "# Define parameter grid for GridSearch\n",
    "param_grid = {\n",
    "    'logreg__C': [0.01, 0.1, 1, 10, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING GridSearchCV to find the best hyperparameters\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the best parameters from GridSearch\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "\n",
    "# extracting the test data\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='binary')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the best logistic regression model i.e. with logreg_C=100 as calculated above\n",
    "\n",
    "best_logreg = grid_search.best_estimator_.named_steps['logreg']\n",
    "\n",
    "# Getting the coefficients and intercept\n",
    "\n",
    "coefficients = best_logreg.coef_[0]\n",
    "intercept = best_logreg.intercept_[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "preprocessor = grid_search.best_estimator_.named_steps['preprocessor']\n",
    "scaled_feature_names = preprocessor.transformers_[0][2]  # Features that were scaled\n",
    "binary_feature_names = preprocessor.transformers_[1][2]  # Features that were not scaled\n",
    "\n",
    "# Combine feature names in the same order they were processed by ColumnTransformer\n",
    "\n",
    "feature_names = list(scaled_feature_names) + list(binary_feature_names)\n",
    "coef_dict = dict(zip(feature_names, coefficients))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## viewing the coefficients obtained\n",
    "\n",
    "print(f\"Intercept: {intercept}\")\n",
    "print(\"Coefficients:\")\n",
    "for feature, coef in coef_dict.items():\n",
    "    print(f\"{feature}: {coef}\")\n",
    "\n",
    "# Saving the weights to a JSON file for it to be used at backend\n",
    "weights = {\n",
    "    'intercept': intercept,\n",
    "    'coefficients': coef_dict\n",
    "}\n",
    "\n",
    "weights_file = 'model_weights_oversampler_new.json'\n",
    "with open(weights_file, 'w') as f:\n",
    "    json.dump(weights, f)\n",
    "\n",
    "print(f\"Weights saved to {weights_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "\n",
    "model_file = 'credit_score_model_oversampler_new2.pkl'\n",
    "joblib.dump(grid_search.best_estimator_, model_file)\n",
    "print(f\"Model saved to {model_file}\")\n",
    "\n",
    "\n",
    "# Saving the scaling parameters, again to be used at backend\n",
    "\n",
    "scaler = preprocessor.named_transformers_['num']\n",
    "scaler_params = {\n",
    "    'name': 'StandardScaler',\n",
    "    'features': {\n",
    "        feature: {\n",
    "            'mean': scaler.mean_[i],\n",
    "            'scale': scaler.scale_[i]\n",
    "        }\n",
    "        for i, feature in enumerate(features_for_scaling)\n",
    "    }\n",
    "}\n",
    "\n",
    "scaler_params_file = 'scaler_params_new_2_final.json'\n",
    "with open(scaler_params_file, 'w') as f:\n",
    "    json.dump(scaler_params, f)\n",
    "\n",
    "print(f\"Scaler parameters saved to {scaler_params_file}\")\n",
    "\n",
    "# Print the names of the scaled features\n",
    "print(\"Scaled features:\", features_for_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examining the features after standard scaling \n",
    "\n",
    "X_train_scaled = preprocessor.fit_transform(X_train)\n",
    "print(\"Samples after scaling:\")\n",
    "print(pd.DataFrame(X_train_scaled, columns=feature_names).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THRESHOLD TUNING\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict probabilities\n",
    "y_probs = best_logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Define a range of thresholds to test\n",
    "thresholds = np.linspace(0.1, 0.9, 9)\n",
    "\n",
    "# Store performance metrics for each threshold\n",
    "results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Apply the threshold to make binary predictions\n",
    "    y_decision = (y_probs >= threshold).astype(int)\n",
    "\n",
    "    # Evaluate the performance\n",
    "    report = classification_report(y_test, y_decision, output_dict=True)\n",
    "    results.append({\n",
    "        'threshold': threshold,\n",
    "        'precision_1': report['1']['precision'],\n",
    "        'recall_1': report['1']['recall'],\n",
    "        'f1-score_1': report['1']['f1-score'],\n",
    "        'support_1': report['1']['support']\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for easier analysis\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results\n",
    "print(results_df)\n",
    "\n",
    "# Find the best threshold based on the highest F1 score for the minority class\n",
    "best_threshold = results_df.loc[results_df['f1-score_1'].idxmax(), 'threshold']\n",
    "print(f'Best threshold: {best_threshold}')\n",
    "\n",
    "# Reapply the best threshold to make final predictions\n",
    "y_decision_best = (y_probs >= best_threshold).astype(int)\n",
    "\n",
    "# Evaluate the model with the best threshold\n",
    "print(classification_report(y_test, y_decision_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Load the saved model\n",
    "model = joblib.load('credit_score_model_oversampler_new2.pkl')\n",
    "\n",
    "# Predict probabilities on the test data using the loaded model\n",
    "# Note: You need to have X_test and y_test available or reload them if needed\n",
    "y_proba_loaded = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Function to generate credit score based on prediction probability\n",
    "def generate_credit_score(proba):\n",
    "    return proba * 1000  # Simple example: scale probability to a score out of 1000\n",
    "\n",
    "# Generate credit scores for the test set\n",
    "credit_scores = generate_credit_score(y_proba_loaded)\n",
    "\n",
    "# Set the credit score thresholds for loan approval\n",
    "credit_score_thresholds = np.linspace(100, 900, 40)\n",
    "\n",
    "# Calculate and print the metrics for each threshold\n",
    "print(\"Metrics for different credit score thresholds:\")\n",
    "\n",
    "for threshold in credit_score_thresholds:\n",
    "    # Predict loan approval based on the credit score threshold\n",
    "    loan_approval_predictions = credit_scores >= threshold\n",
    "    \n",
    "    # Calculate the F1 scores for the current threshold\n",
    "    f1 = f1_score(y_test, loan_approval_predictions, average='binary')\n",
    "    f1_per_class = f1_score(y_test, loan_approval_predictions, average=None)\n",
    "    \n",
    "    # Calculate the confusion matrix for the current threshold\n",
    "    conf_matrix = confusion_matrix(y_test, loan_approval_predictions)\n",
    "    \n",
    "    # Generate the classification report for detailed metrics\n",
    "    #report = classification_report(y_test, loan_approval_predictions, target_names=['Rejected', 'Approved'], output_dict=True)\n",
    "    \n",
    "    # Print the metrics for the current threshold\n",
    "    print(f\"\\nThreshold = {threshold}\")\n",
    "    print(f\"F1 Score (overall) = {f1:.4f}\")\n",
    "    print(f\"F1 Score (class 0) = {f1_per_class[0]:.4f}\")\n",
    "    print(f\"F1 Score (class 1) = {f1_per_class[1]:.4f}\")\n",
    "    #print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    #print(f\"Classification Report:\\n{report}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
